# -*- coding: utf-8 -*-
"""Forest-Fire-Area-Classification-NASA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-RUDRZ72uzODdhXOuWSwwUZ8epIrlQa5
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import math
import numpy as np
import pandas as pd
import random


from sklearn import neural_network, linear_model, preprocessing, svm, tree
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.naive_bayes import GaussianNB


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
from scikeras.wrappers import KerasRegressor


import warnings


warnings.filterwarnings(action="ignore", module="scipy", message="^internal gelsd")
# %matplotlib inline



forest_fires = pd.read_csv(r"D:\Excel\MODIS_C6_1_Global_24h.csv")
forest_fires.head(10)

forest_fires.daynight.replace(('D','N'),(1,2), inplace=True)
#forest_fires.version.replace(('6.0NRT'),(1), inplace=True)
forest_fires.satellite.replace(('T','A'),(1,2), inplace=True)
forest_fires = forest_fires.drop(labels=["version","acq_date","acq_time"],axis="columns")

forest_fires

forest_fires.describe()

"""#### If fire area > 0, set the value to 1 and change column name from area to label so that we can see it as a classification problem"""

forest_fires['confidence'].values[forest_fires['confidence'].values <50] = 0
forest_fires['confidence'].values[forest_fires['confidence'].values >=50 ] = 1

forest_fires = forest_fires.rename(columns={'confidence': 'label'})
forest_fires

forest_fires.corr()

"""### We can find the third highest correlation coefficients would be "brightness", "bright_t31", "frp"
"""

forest_fires.corr()['label'].sort_values(ascending=False)



"""# Machine Learning

### Logistic Regression Classification
"""

import pandas as pd
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scaler = StandardScaler()

scaler.fit(forest_fires.drop('label',axis=1))

scaled_features = scaler.transform(forest_fires.drop('label',axis=1))
df_feat = pd.DataFrame(scaled_features,columns=forest_fires.columns[:-1])
df_feat.head()

X = df_feat
y = forest_fires['label']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)

logmodel = LogisticRegression(solver='liblinear')
logmodel.fit(X_train,y_train)

predictions = logmodel.predict(X_test)

from sklearn import metrics
logmodel.score(X_train,y_train)
print("Accuracy:",metrics.accuracy_score(y_test, predictions))
print("Precision:",metrics.precision_score(y_test, predictions))
print("Recall:",metrics.recall_score(y_test, predictions))


from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,predictions))
print(classification_report(y_test,predictions))

import joblib

# Save the Logistic Regression model
joblib.dump(logmodel, 'logistic_regression_model.pkl')

# Save the StandardScaler
joblib.dump(scaler, 'scaler.pkl')

classes={0:'safe',1:'On Fire'}
#x_new=[[-17.964, -49.077, 308.2 ,3.5 ,1.8, 1,291.7, 48.8, 2]]
x_new=[[-17.948, -49.079, 306.3 ,3.5 ,1.8, 1,291.8, 39.8, 2]]
y_predict=logmodel.predict(x_new)
print(classes[y_predict[0]])

"""# K-Nearest"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(forest_fires.drop('label',axis=1))
scaled_features = scaler.transform(forest_fires.drop('label',axis=1))
df_feat = pd.DataFrame(scaled_features,columns=forest_fires.columns[:-1])
df_feat.head()

from sklearn.model_selection import train_test_split
X = df_feat
y = forest_fires['label']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)
pred = knn.predict(X_test)

error_rate = []

for i in range(1,60):

    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))



plt.figure(figsize=(10,6))
plt.plot(range(1,60),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

knn = KNeighborsClassifier(n_neighbors=15)
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
print('WITH K=15')
print('\n')
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))

knn = KNeighborsClassifier(n_neighbors=17)
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
print('WITH K=17')
print('\n')
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))

knn.score(X_test, y_test)

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(y_test, pred))
print("Precision:",metrics.precision_score(y_test, pred))
print("Recall:",metrics.recall_score(y_test, pred))

classes={0:'safe',1:'On Fire'}
#x_new=[[1, 4, 9 ,1 ,91.5, 130.1, 807.1, 7.5, 21.3, 35, 2.2, 0]]
#x_new=[[-16.412, -49.149, 303.3 ,3.4 ,1.7, 1,291.6, 26.9, 2]]
x_new=[[-17.964, -49.077, 308.2 ,3.5 ,1.8, 1,291.7, 48.8, 2]]

y_predict=knn.predict(x_new)
print(classes[y_predict[0]])

"""# SVM"""

from sklearn import metrics
from sklearn.svm import SVC


X = forest_fires.drop('label', axis=1)
y = forest_fires['label']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=101)

svc = SVC()
svc.fit(X_train, y_train)

prediction = svc.predict(X_test)

print(metrics.classification_report(y_test, prediction))
print(metrics.confusion_matrix(y_test, prediction))

print("Accuracy:",metrics.accuracy_score(y_test, prediction))
print("Precision:",metrics.precision_score(y_test, prediction))
print("Recall:",metrics.recall_score(y_test, prediction))

classes={0:'safe',1:'On Fire'}
#x_new=[[1, 4, 9 ,1 ,91.5, 130.1, 807.1, 7.5, 21.3, 35, 2.2, 0]]
x_new=[[-17.964, -49.077, 308.2 ,3.5 ,1.8, 1,291.7, 48.8, 2]]
y_predict=svc.predict(x_new)
print(classes[y_predict[0]])

import pickle
pickle_out = open("svc.pkl", "wb")
pickle.dump(svc, pickle_out)
pickle_out.close()

"""# Decision Tree"""

from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier

X = forest_fires.drop('label', axis=1)
y = forest_fires['label']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=101)

d_tree = DecisionTreeClassifier()
d_tree.fit(X_train, y_train)

predicted = d_tree.predict(X_test)

print(metrics.classification_report(y_test, predicted))
print(metrics.confusion_matrix(y_test, predicted))

print("Accuracy:",metrics.accuracy_score(y_test, predicted))
print("Precision:",metrics.precision_score(y_test, predicted))
print("Recall:",metrics.recall_score(y_test, predicted))

classes={0:'safe',1:'On Fire'}
#x_new=[[1, 4, 9 ,1 ,91.5, 130.1, 807.1, 7.5, 21.3, 35, 2.2, 0]]
x_new=[[-17.964, -49.077, 308.2 ,3.5 ,1.8, 1,291.7, 48.8, 2]]
y_predict=d_tree.predict(x_new)
print(classes[y_predict[0]])

from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
X = forest_fires.drop('label', axis=1)
y = forest_fires['label']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=101)

# fit a k-nearest neighbor model to the data
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

print(knn)
# make predictions
predicted = knn.predict(X_test)
# summarize the fit of the model
print(metrics.classification_report(y_test, predicted))
print(metrics.confusion_matrix(y_test, predicted))

print("Accuracy:",metrics.accuracy_score(y_test, predicted))
print("Precision:",metrics.precision_score(y_test, predicted))
print("Recall:",metrics.recall_score(y_test, predicted))

classes={0:'safe',1:'On Fire'}
#x_new=[[1, 4, 9 ,1 ,91.5, 130.1, 807.1, 7.5, 21.3, 35, 2.2, 0]]
x_new=[[-17.964, -49.077, 308.2 ,3.5 ,1.8, 1,291.7, 48.8, 2]]
y_predict=knn.predict(x_new)
print(classes[y_predict[0]])

# Gaussian Naive Bayes
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB

X = forest_fires.drop('label', axis=1)
y = forest_fires['label']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=101)

# fit a Naive Bayes model to the data
G_NB = GaussianNB()
G_NB.fit(X_train,y_train)
print(G_NB)


predict = G_NB.predict(X_test)

print(metrics.classification_report(y_test, predict))
print(metrics.confusion_matrix(y_test, predict))

print("Accuracy:",metrics.accuracy_score(y_test, predict))
print("Precision:",metrics.precision_score(y_test, predict))
print("Recall:",metrics.recall_score(y_test, predict))

classes={0:'safe',1:'On Fire'}
#x_new=[[1, 4, 9 ,1 ,91.5, 130.1, 807.1, 7.5, 21.3, 35, 2.2, 0]]
x_new=[[-17.964, -49.077, 308.2 ,3.5 ,1.8, 120, 1,291.7, 48.8, 2]]
y_predict=G_NB.predict(x_new)
print(classes[y_predict[0]])



# Compare Algorithms
import pandas
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier

# load dataset
X = forest_fires.drop('label', axis=1)
y = forest_fires['label']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=101)

# prepare configuration for cross validation test harness
seed = 7
# prepare models
models = []
models.append(('LR', LogisticRegression(max_iter=5000)))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
models.append(('MLP', MLPClassifier()))
models.append(('GradientBoost',GradientBoostingClassifier()))
models.append(('AdaBoost',AdaBoostClassifier()))
models.append(('Bagging',BaggingClassifier()))
models.append(('RandomForest',RandomForestClassifier()))
models.append(('ExtraTrees',ExtraTreesClassifier()))

# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
    kfold = model_selection.KFold(n_splits=10)
    cv_results = model_selection.cross_val_score(model, X,y,   cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

